<!doctype html>



  


<html class="theme-next mist use-motion" lang="">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.0" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Hexo, NexT" />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.0" />






<meta name="description" content="salty fish jojo23333&apos;s personal page">
<meta property="og:type" content="website">
<meta property="og:title" content="JonassenLi">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="JonassenLi">
<meta property="og:description" content="salty fish jojo23333&apos;s personal page">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="JonassenLi">
<meta name="twitter:description" content="salty fish jojo23333&apos;s personal page">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":0,"b2t":false,"scrollpercent":false},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/"/>





  <title> JonassenLi </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="">

  














  
  
    
  

  <div class="container one-collumn sidebar-position-left 
   page-home 
 ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">JonassenLi</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">LMC's Notebook</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Startseite
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Kategorien
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archiv
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/09/17/sensetime/STTN-tf-Implementation/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="JonassenLi">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/cat.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="JonassenLi">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2018/09/17/sensetime/STTN-tf-Implementation/" itemprop="url">
                  STTN tf Implementation
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-09-17T17:03:36+08:00">
                2018-09-17
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="STTN-tensorflow-实现"><a href="#STTN-tensorflow-实现" class="headerlink" title="STTN tensorflow 实现"></a>STTN tensorflow 实现</h1><h2 id="前要"><a href="#前要" class="headerlink" title="前要"></a>前要</h2><p>这篇文章是论文 <a href="http://openaccess.thecvf.com/content_ECCV_2018/html/Tae_Hyun_Kim_Spatio-temporal_Transformer_Network_ECCV_2018_paper.html" target="_blank" rel="external">Spatio-Temporal Transformer Network for Video Restoration.</a>的实现，之前有写过这篇论文的解析 <a href="../STTN-论文笔记">传送门</a><br>第一次尝试用tensorflow复现论文..（；´д｀）ゞ而且还是从头写，虽然之前也片片段段的写过一点，不过从头写感觉就是不一样啊,这里先贴一点我用到的资料吧。  </p>
<ul>
<li>我发现tensorflow所有文档里面除了api以外..<a href="https://tensorflow.google.cn/programmers_guide/" target="_blank" rel="external">guide</a>最好用..</li>
<li>STN –&gt; STNN STN Offical Repo: <a href="https://github.com/kevinzakka/spatial-transformer-network" target="_blank" rel="external">https://github.com/kevinzakka/spatial-transformer-network</a></li>
<li>尝试理解一个构造模式也不错，由于原论文refer到了我参考了VDSR 的 tensorflow-repo: <a href="https://github.com/Jongchan/tensorflow-vdsr" target="_blank" rel="external">https://github.com/Jongchan/tensorflow-vdsr</a></li>
</ul>
<h2 id="network-architecture"><a href="#network-architecture" class="headerlink" title="network architecture"></a>network architecture</h2><p>虽然network architecture在之前一篇文章里面也有提到过，这里就提一些细节吧，整体的网络结构和</p>
<ul>
<li>U—net 包括下采样层和上采样层两个部分，与U-net论文里提到的U-net不同，这里的U-net下采样通过stride=2的卷积层来做到，而不是池化层。而上采样使用的是双线性插值。（这里面的两点区别有待仔细考虑，例如用反卷积会怎么样？）</li>
<li>Spatio-Temporal Sampler 实现类似于STN（其实觉得这种能自动算gradient很神奇，有时间仔细观察一下tensorboard理解一下这里咋back propoagate）</li>
<li>Image proccesing part 我就简单的使用了他video restoration network 里面的resnet-9</li>
</ul>
<p>tensorboard 可视化之后结果大概这样：  </p>
<table>
<thead>
<tr>
<th style="text-align:left">unet部分</th>
<th style="text-align:left">res9部分</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><img src="/images/sttn/unet.png" alt=""></td>
<td style="text-align:left"><img src="/images/sttn/res9.png" alt=""></td>
</tr>
</tbody>
</table>
<h2 id="有关数据集"><a href="#有关数据集" class="headerlink" title="有关数据集"></a>有关数据集</h2><h3 id="Self-made-VideoScenes-数据集"><a href="#Self-made-VideoScenes-数据集" class="headerlink" title="Self-made VideoScenes 数据集"></a>Self-made VideoScenes 数据集</h3><p>这个论文既不公开代码也不公开数据集=。=。我参考了这篇论文refer的一些论文用的数据集，发现很多都是通过下载网站上的HD Video来搞的。然后同样是参考了这篇论文refer的论文的一些数据。发现它download的一些数据集大多是一些运动类的高清数据集。于是我也采用了类似的数据集（不过video还是自己手动一个一个去download的）。<br>下好20多个时长在5~10分钟不等的video后，我发现一个严重的问题，那就是这些视频大多数由多个连续镜头剪辑在一起形成。而且有的还有片头片尾，我必须要考虑如下的一些事情：</p>
<ol>
<li><p><strong>场景分割</strong>。 不能简单地把视频转化为帧，必须要考虑场景和场景之间在哪切分，不然训练到场景切换之间地数据就有问题<br>幸运的是，python有一个scenedetect的工具来处理这个问题，具体涉及到一些阈值的处理方式啥的可以参考<a href="https://pyscenedetect.readthedocs.io/en/latest/" target="_blank" rel="external">文档</a></p>
</li>
<li><p><strong>分割间隔</strong>。 视频取帧怎么设置间隔？<br>我觉得这是个很难说的问题，因为有缓慢地视角移动也有快速的视角移动，但是按理来说，STTN的optical flow estimation Network应该是要能学到光流中包含的这些信息的，暂时来说我是取了0.1s为间隔</p>
</li>
<li><p><strong>数据清洗</strong><br>有一些渐暗的视频切换效果比较难鉴别到，也有一些制作人表之类的Scene需要自己去掉</p>
</li>
<li><p><strong>数据加噪</strong><br>宇哥跟我建议先加高斯噪声试试..我就先每个frame resize之后加的高斯噪声..我觉得之后需要加上blur和down sample弄复杂一点..</p>
</li>
</ol>
<p>意思意思贴一点数据处理的代码，估计以后还用的上PySceneDetect这个工具<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">split_video_by_scene</span><span class="params">(video)</span>:</span></div><div class="line">    video_name = video.split(<span class="string">'.'</span>)[<span class="number">-2</span>].split(<span class="string">'/'</span>)[<span class="number">-1</span>]</div><div class="line">    os.system(<span class="string">"scenedetect --input \""</span> + video +  <span class="string">"\" list-scenes detect-content -t 25"</span>)</div><div class="line"></div><div class="line">    scene = pd.read_csv(video_name + <span class="string">"-Scenes.csv"</span>, skiprows=<span class="number">1</span>)</div><div class="line">    scene_begin = list(scene[<span class="string">"Start Time (seconds)"</span>])</div><div class="line">    scene_end = list(scene[<span class="string">"End Time (seconds)"</span>])</div><div class="line">    end_frame = scene_end[<span class="number">-1</span>]</div><div class="line">    save_scene_images(video, scene_begin, scene_end, end_frame)</div></pre></td></tr></table></figure></p>
<h3 id="GoPR-数据集"><a href="#GoPR-数据集" class="headerlink" title="GoPR 数据集"></a>GoPR 数据集</h3><p>这个数据集是CVPR2017的一篇论文用到的数据集，具体戳 <a href="https://github.com/SeungjunNah/DeepDeblur_release" target="_blank" rel="external">https://github.com/SeungjunNah/DeepDeblur_release</a><br>大概有2k张连续帧，1k多张测试用，其实很小了..<br>更大的数据集估计能有1w张清晰图，之后参考那个</p>
<h2 id="实现当中踩过的一些坑和问题"><a href="#实现当中踩过的一些坑和问题" class="headerlink" title="实现当中踩过的一些坑和问题"></a>实现当中踩过的一些坑和问题</h2><p>### </p>
<h2 id="Train过程中遇到的问题汇总"><a href="#Train过程中遇到的问题汇总" class="headerlink" title="Train过程中遇到的问题汇总"></a>Train过程中遇到的问题汇总</h2><ol>
<li>一开始Train不起来 :(<br>可能太年轻，一个1280<em> 720 </em> 3 的tensor,其对应的一个conv2d没想到竟然会占到450M的显存，分分钟爆掉11G的1080Ti….,batch_size=1才勉强能跑。无奈只好downsize输入，1280 <em> 720 -&gt; 640 </em> 360。<br>为了保证训练的数据集像素不变，每次训练随机地crop 640*360大小的图片出来然后feed进去。由于数据集本身不是很大，这种方法应该也从某种意义上扩大了数据集？（在原数据集上random crop）</li>
<li>能训练了之后，针对自己的数据集训练的第一版效果不是很好，很多有噪声的地方被模糊化了，估计是optical flow network没有训练好。用tensorboard加上了一些输出信息，观察到正则loss占了很大部分比重，减小正则项，同时使用decayrate 来减少有时候出现的diverge现象。以上均属于hyperparameter finetuning 过程。</li>
<li>用并行输入优化，尝试加快训练过程。由于现在仍是在单卡上训练，后续考虑tensorflow的多卡数据并行训练。</li>
<li>怀疑之前的问题很大程度是光流网络输出的问题。尝试visualize光流网络的训练结果。同时考虑替换后续image processing network为densenet。</li>
<li>有关loss, 论文里没明确给loss的表达式子，现在是joint train两个loss ,xt~ yt~相对于ground_truth的loss ,xt~ yt~如下图<br><img src="/images/sttn/architecture2.png" alt=""></li>
</ol>
<h2 id="训练结果记录"><a href="#训练结果记录" class="headerlink" title="训练结果记录"></a>训练结果记录</h2><h3 id="GoPR数据集第一批的训练结果"><a href="#GoPR数据集第一批的训练结果" class="headerlink" title="GoPR数据集第一批的训练结果"></a>GoPR数据集第一批的训练结果</h3><p>psnr基本在25上下，下面是一个结果截图<br><img src="/images/sttn/result_analyze1.png" alt="一个效果一般的结果"></p>
<ol>
<li>效果一般，但是可以看到网络似乎是学到了通过预测光流用其它帧来补偿模糊(人脚那一块的模糊)</li>
<li>考虑改善后处理网络</li>
<li>我plot了一下total loss 和每训练一个epoch得到的test loss如下图所示  </li>
</ol>
<table>
<thead>
<tr>
<th style="text-align:left">train_mse</th>
<th style="text-align:left">test_mse</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><img src="/images/sttn/total_mse.png" alt=""></td>
<td style="text-align:left"><img src="/images/sttn/test_mse.png" alt=""></td>
</tr>
</tbody>
</table>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/09/15/sensetime/STTN-论文笔记/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="JonassenLi">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/cat.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="JonassenLi">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2018/09/15/sensetime/STTN-论文笔记/" itemprop="url">
                  STTN-论文笔记
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-09-15T18:28:09+08:00">
                2018-09-15
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">in</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/论文笔记/" itemprop="url" rel="index">
                    <span itemprop="name">论文笔记</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="论文笔记-STTN-ECCV-2018"><a href="#论文笔记-STTN-ECCV-2018" class="headerlink" title="论文笔记-STTN ECCV 2018"></a>论文笔记-STTN ECCV 2018</h1><h2 id="前要"><a href="#前要" class="headerlink" title="前要"></a>前要</h2><p>刚来sensetime的第一天，在工位上不知所措，下午挑了几篇单目深度估计的文章来看，然后…宇哥晚上跟我说来来来你把这篇文章看一看吧，然后就有了第一个任务。  </p>
<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>这篇文章是ECCV2018的一篇文章 <a href="http://openaccess.thecvf.com/content_ECCV_2018/html/Tae_Hyun_Kim_Spatio-temporal_Transformer_Network_ECCV_2018_paper.html" target="_blank" rel="external">Spatio-Temporal Transformer Network for Video Restoration.</a><br>现在state of art的Video Restoration Method 通常使用了optical flow network来优化视频中帧与帧之间的短时的信息。然而这些方法大多数只关注相邻的一对帧之间的联系，从而忽略了视频中较长距离的帧之间的联系。这篇文章提出了一种网络结构（Spatio-Temporal Transformer Network）能够一次性处理多帧，从而解决了视频中的遮挡问题，也可以应用于视频超分辨率和视频去模糊等其它问题。</p>
<h2 id="Main-Idea"><a href="#Main-Idea" class="headerlink" title="Main Idea"></a>Main Idea</h2><p>这篇文章的Inspiration来自于Google的一篇文章<a href="https://arxiv.org/abs/1506.02025" target="_blank" rel="external">Spatial Transformer Networks</a><br>STN网络的实质就是训练了一个Grid Generator 来对原图进行变化，或者说对原图重新Sample<br>见下图与对应公式，这样做的好处在于，弥补了神经网络对空间不变性的缺陷(spatial invariant),比如说对于下图的手写数字，重新采样后的图片一定程度上恢复了数字的旋转压缩，这让后面神经网络的准确率大大增加。</p>
<p>STTN采用了STN的思想，把二维扩展到了三维。原STN是通过预测一个二维的grid generator来生成采样点，而STTN则是通过预测多帧之间的Flow(可以理解为光流？)来确定一个在多帧之间的采样点。<br>有关STN 可以参考这里：<a href="https://kevinzakka.github.io/2017/01/18/stn-part2/" target="_blank" rel="external">https://kevinzakka.github.io/2017/01/18/stn-part2/</a></p>
<h2 id="Architect-Detail"><a href="#Architect-Detail" class="headerlink" title="Architect Detail"></a>Architect Detail</h2><p>STTN network 的网络结构如下图所示<br><img src="/images/sttn/architecture.png" alt="STTN 网络结构"></p>
<h3 id="spatio-Temporal-Flow-Estimation-Network"><a href="#spatio-Temporal-Flow-Estimation-Network" class="headerlink" title="spatio-Temporal Flow Estimation Network"></a>spatio-Temporal Flow Estimation Network</h3><p>传统的预测光流的方法常用相邻两张图像比较，比较多次之后得到结果，一是计算耗时，二是不可靠。<br>STTN使用了一种<a href="https://arxiv.org/abs/1505.04597v1" target="_blank" rel="external">U-net</a>的网络结构，将所有帧stack到一起（H<em>W</em>C<em>T）作为网络的输入，输出(u,v,w)-&gt;(H</em>W*3)的光流<br>U-net的网络结构如下所示</p>
<h3 id="Differentiable-Spatio-Temporal-Sampler"><a href="#Differentiable-Spatio-Temporal-Sampler" class="headerlink" title="Differentiable Spatio-Temporal Sampler"></a>Differentiable Spatio-Temporal Sampler</h3><p>这一块和STN中的Grid Generator相同，根据得到的Optical Flow对原图进行采样。公式如下<br><img src="/images/sttn/formu_1.png" alt=""><br>这个公式看着唬人，实际上比STN的思想还要暴力简单,展开之后<br><img src="/images/sttn/formu_2.png" alt=""><br>实际上想一想，就是把每一个点(x,y,t) 分别加上(u,v,w)的偏移量之后得到的新点，根据到其空间内最近四个点的距离加权求和</p>
<h3 id="Image-Processing-part"><a href="#Image-Processing-part" class="headerlink" title="Image Processing part"></a>Image Processing part</h3><p>原图给了一个Video restoration的例子，使用的了Resblock*9? sttn结构这个东西好像可以配合各种网络用上，如下图所示。<br><img src="/images/sttn/architecture2.png" alt=""></p>
<h2 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h2><p>这一篇文章目前好像还没有放出官方代码，数据集也不见踪影,更坑爹的是loss function和各种Test data给的十分不详细…然鹅宇哥让我实现一下(- ▽ -)”…<br>Tensorflow实现与分析见另一篇文章 <a href="../STTN-tf-Implementation/">传送门</a></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/04/15/machinelearning/tensorflow_notes/style_transfer/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="JonassenLi">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/cat.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="JonassenLi">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2018/04/15/machinelearning/tensorflow_notes/style_transfer/" itemprop="url">
                  Image Style Transfer based on CNN
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-04-15T20:18:00+08:00">
                2018-04-15
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="实验提要"><a href="#实验提要" class="headerlink" title="实验提要"></a>实验提要</h2><p>刚做完CS20的assignmet 2，因为是第一个tensorflow项目，虽然很多不知道怎么做借鉴了别人的代码，整个代码框架大致是搞懂了，姑且留个记录。</p>
<p>整个实验基本上是对<a href="https://arxiv.org/pdf/1508.06576.pdf" target="_blank" rel="external">A Neural Algorithm of Artistic Style</a><br>这篇文章的一个实现。实验提供了一些框架代码，可以在<a href="https://github.com/chiphuyen/stanford-tensorflow-tutorials/tree/master/assignments/02_style_transfer" target="_blank" rel="external">git上这里</a>找到</p>
<h2 id="原论文以及主要观点"><a href="#原论文以及主要观点" class="headerlink" title="原论文以及主要观点"></a>原论文以及主要观点</h2><h3 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h3><p>A Neural Algorithm of Artistic Style 这篇文章发表2016，还算比较新的文章。<br>文章的主要点在于它发现了在CNN当中图片的内容和图片的风格是可以分离的，因而可以独立的处理这些表示生成新的有意义的图片（虽然我也没完全弄懂他的意思），原文如下：</p>
<blockquote>
<p>“The key finding of this paper is that the representations of content and style in the Convolutional Neural Network are separable. That is, we can manipulate both representations independently to produce new, perceptually meaningful images.”</p>
</blockquote>
<h3 id="VGG-Network-结构"><a href="#VGG-Network-结构" class="headerlink" title="VGG-Network 结构"></a>VGG-Network 结构</h3><p>文章使用的实现方法基于VGG-Network，在cs231n的<a href="http://cs231n.github.io/convolutional-networks/" target="_blank" rel="external">这个课件</a>里有对VGG-Net的简要介绍    </p>
<blockquote>
<p>VGGNet. The runner-up in ILSVRC 2014 was the network from Karen Simonyan and Andrew Zisserman that became known as the VGGNet. Its main contribution was in showing that the depth of the network is a critical component for good performance. Their final best network contains 16 CONV/FC layers and, appealingly, <strong>features an extremely homogeneous architecture that only performs 3x3 convolutions and 2x2 pooling from the beginning to the end.</strong> Their pretrained model is available for plug and play use in Caffe. A downside of the VGGNet is that it is more expensive to evaluate and uses a lot more memory and parameters (140M). Most of these parameters are in the first fully connected layer, and it was since found that these FC layers can be removed with no performance downgrade, significantly reducing the number of necessary parameters.</p>
</blockquote>
<p>VGGnet的网络参数如图所示  </p>
<p><img src="/images/tf/VGGNet.png" alt="VGGNet achitecture"></p>
<h3 id="基于VGGnet-的实现"><a href="#基于VGGnet-的实现" class="headerlink" title="基于VGGnet 的实现"></a>基于VGGnet 的实现</h3><p>文章使用了VGGNet当中的16层卷积层和5层pooling层,去掉了全连接层，并使用average pooling策略替换max pooling策略</p>
<h4 id="lose-function"><a href="#lose-function" class="headerlink" title="lose function"></a>lose function</h4><p>关于怎么定义loss function,想法比较自然<br>在图像内容附近通过白噪声初始化一个输出的结果，然后通过网络对这个结果进行风格和内容两方面的约束进行修正。<br><strong>content loss</strong><br>设置一个白点噪声的初始图像和原图像输入网络，在某一层的输出$l$处,F和P分别为其特征表述，则取其方差为content loss</p>
<p>$$ L_{content}(\vec p,\vec x, l) = \frac{1}{2}\sum_{i,j}(F_{ij}^{l}-P_{ij}^{l})^2 $$</p>
<p><strong>Gram矩阵</strong></p>
<p>Gram Matrix实际上可看做是feature之间的偏心协方差矩阵（即没有减去均值的协方差矩阵）<br>协方差矩阵可写成：</p>
<p>$$ \sum  = E[(X-E(X))(X-E(X))^T]$$<br>Gram矩阵可写成</p>
<p>$$ G = A * A^{T} $$</p>
<p><strong>style loss</strong>  </p>
<p>在CNN每一层反馈的基础上，对每一层的激励结果求其Gram矩阵,同样是对生成图像和原图像，在某一层l生成的两个Gram矩阵G、A<br>这一层loss贡献为：</p>
<p>$$ E_l = \frac{1}{4N_l^2M_l^2}\sum_{i,j}(G_{ij}^l-A_{ij}^l)^2$$</p>
<p>对每一层的loss进行加权求和，得到总的loss为</p>
<p>$$ L_{style}(\vec a,\vec x) = \sum_{l=0}^Lw_lE_l$$</p>
<p>给定content loss和style loss分别的权重为$\alpha$和$\beta$，总的优化目标为</p>
<p>$$ L_{total}(\vec p,\vec a,\vec x)=\alpha L_{content}(\vec p,\vec x)+\beta L_{style}(\vec a,\vec x)$$</p>
<p><strong>思考</strong><br>有关于为什么要使用gram matrix来度量风格，当同一个维度上面的值相乘的时候原来越小酒变得更小，原来越大就变得越大，二不同维度上的关系也在相乘的表达当中表示出来,因而gram matrix能有效度量各个维度自己的特性以及各个维度之间的关系</p>
<h2 id="基于tensorflow的实现"><a href="#基于tensorflow的实现" class="headerlink" title="基于tensorflow的实现"></a>基于tensorflow的实现</h2><h3 id="代码框架"><a href="#代码框架" class="headerlink" title="代码框架"></a>代码框架</h3><ul>
<li>utils.py  一些辅助函数</li>
<li>load_vgg.py  从已经训练好的参数当中加载vggnet</li>
<li>style_transfer.py 构建风格转化的模型</li>
</ul>
<h3 id="load-vgg-py"><a href="#load-vgg-py" class="headerlink" title="load_vgg.py"></a>load_vgg.py</h3><p>这个模块中的主要任务是搭建vggnet，在load方法当中调用conv2d_relu生成卷积层，调用avgpool生成pooling层<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv2d_relu</span><span class="params">(self, prev_layer, layer_idx, layer_name)</span>:</span></div><div class="line">    <span class="keyword">with</span> tf.variable_scope(layer_name):</div><div class="line">                w, b = self._weights(layer_idx, layer_name)</div><div class="line">                w = tf.constant(w, name=<span class="string">"weight"</span>)</div><div class="line">                b = tf.constant(b, name=<span class="string">"bais"</span>)</div><div class="line">                conv2d = tf.nn.conv2d(input = prev_layer,</div><div class="line">                                    filter = w,</div><div class="line">                                    strides = [<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>],</div><div class="line">                                    padding = <span class="string">"SAME"</span>,</div><div class="line">                                    name = layer_name)</div><div class="line">                out = tf.nn.relu(conv2d + b)</div><div class="line">        setattr(self, layer_name, out)</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">avgpool</span><span class="params">(self, prev_layer, layer_name)</span>:</span></div><div class="line"></div><div class="line">        <span class="keyword">with</span> tf.variable_scope(layer_name):</div><div class="line">            out = tf.nn.avg_pool(prev_layer,</div><div class="line">                                ksize=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>],</div><div class="line">                                strides=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>],</div><div class="line">                                padding=<span class="string">"SAME"</span>)</div><div class="line"></div><div class="line">        setattr(self, layer_name, out)</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">load</span><span class="params">(self)</span>:</span></div><div class="line">        self.conv2d_relu(self.input_img, <span class="number">0</span>, <span class="string">'conv1_1'</span>)</div><div class="line">        self.conv2d_relu(self.conv1_1, <span class="number">2</span>, <span class="string">'conv1_2'</span>)</div><div class="line">        self.avgpool(self.conv1_2, <span class="string">'avgpool1'</span>)</div><div class="line">        self.conv2d_relu(self.avgpool1, <span class="number">5</span>, <span class="string">'conv2_1'</span>)</div><div class="line">        self.conv2d_relu(self.conv2_1, <span class="number">7</span>, <span class="string">'conv2_2'</span>)</div><div class="line">        self.avgpool(self.conv2_2, <span class="string">'avgpool2'</span>)</div><div class="line">        self.conv2d_relu(self.avgpool2, <span class="number">10</span>, <span class="string">'conv3_1'</span>)</div><div class="line">        self.conv2d_relu(self.conv3_1, <span class="number">12</span>, <span class="string">'conv3_2'</span>)</div><div class="line">        self.conv2d_relu(self.conv3_2, <span class="number">14</span>, <span class="string">'conv3_3'</span>)</div><div class="line">        self.conv2d_relu(self.conv3_3, <span class="number">16</span>, <span class="string">'conv3_4'</span>)</div><div class="line">        self.avgpool(self.conv3_4, <span class="string">'avgpool3'</span>)</div><div class="line">        self.conv2d_relu(self.avgpool3, <span class="number">19</span>, <span class="string">'conv4_1'</span>)</div><div class="line">        self.conv2d_relu(self.conv4_1, <span class="number">21</span>, <span class="string">'conv4_2'</span>)</div><div class="line">        self.conv2d_relu(self.conv4_2, <span class="number">23</span>, <span class="string">'conv4_3'</span>)</div><div class="line">        self.conv2d_relu(self.conv4_3, <span class="number">25</span>, <span class="string">'conv4_4'</span>)</div><div class="line">        self.avgpool(self.conv4_4, <span class="string">'avgpool4'</span>)</div><div class="line">        self.conv2d_relu(self.avgpool4, <span class="number">28</span>, <span class="string">'conv5_1'</span>)</div><div class="line">        self.conv2d_relu(self.conv5_1, <span class="number">30</span>, <span class="string">'conv5_2'</span>)</div><div class="line">        self.conv2d_relu(self.conv5_2, <span class="number">32</span>, <span class="string">'conv5_3'</span>)</div><div class="line">        self.conv2d_relu(self.conv5_3, <span class="number">34</span>, <span class="string">'conv5_4'</span>)</div><div class="line">        self.avgpool(self.conv5_4, <span class="string">'avgpool5'</span>)</div></pre></td></tr></table></figure></p>
<p>有关conv2d的参数解释见之前笔记和<a href="https://www.tensorflow.org/api_docs/python/tf/nn/conv2d" target="_blank" rel="external">这里</a></p>
<h3 id="style-transfer"><a href="#style-transfer" class="headerlink" title="style_transfer"></a>style_transfer</h3><p>总体分为两部，第一步创建tensorflow图结构，这其中包括：</p>
<ol>
<li>使用create_input创建空白图作为输入</li>
<li>加载vggnet结构</li>
<li>创建loss</li>
<li>根据loss创建optimizer</li>
<li>创建统计数据<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">build</span><span class="params">(self)</span>:</span></div><div class="line">    self.create_input()</div><div class="line">    self.load_vgg()</div><div class="line">    self.losses()</div><div class="line">    self.optimize()</div><div class="line">    self.create_summary()</div></pre></td></tr></table></figure>
</li>
</ol>
<p>第二步执行训练:  </p>
<ol>
<li>初始化全局变量sess.run(tf.global_variables_initializer())</li>
<li>创建FileWriter (用于TensorBoard)</li>
<li>创建输入  sess.run(self.input_img.assign(self.initial_img))</li>
<li>创建checkpoint检查是否要恢复</li>
<li>循环迭代n次</li>
<li>最优化opt</li>
<li>每特定次循环计算保存summary，保存断点<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self, n_iters)</span>:</span></div><div class="line">    skip_step = <span class="number">1</span></div><div class="line">    <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">        <span class="comment"># 1. initialize</span></div><div class="line">        sess.run(tf.global_variables_initializer())</div><div class="line">        <span class="comment"># 2. create writer</span></div><div class="line">        writer = tf.summary.FileWriter(<span class="string">"graphs/style_transfer"</span>, sess.graph)</div><div class="line">        <span class="comment"># 3. assign input</span></div><div class="line">        sess.run(self.input_img.assign(self.initial_img))</div><div class="line">        <span class="comment"># 4. create checkpoint &amp; restore previous</span></div><div class="line">        saver = tf.train.Saver()</div><div class="line">        ckpt = tf.train.get_checkpoint_state(os.path.dirname(<span class="string">'checkpoints/style_transfer/checkpoint'</span>))</div><div class="line">        <span class="keyword">if</span> ckpt <span class="keyword">and</span> ckpt.model_checkpoint_path:</div><div class="line">            saver.restore(sess, ckpt.model_checkpoint_path)</div><div class="line"></div><div class="line">        initial_step = self.gstep.eval()</div><div class="line">        start_time = time.time()</div><div class="line">        <span class="comment"># 5. iterate for n_iters time</span></div><div class="line">        <span class="keyword">for</span> index <span class="keyword">in</span> range(initial_step, n_iters):</div><div class="line">            <span class="keyword">if</span> index &gt;= <span class="number">5</span> <span class="keyword">and</span> index &lt; <span class="number">20</span>:</div><div class="line">                skip_step = <span class="number">10</span></div><div class="line">            <span class="keyword">elif</span> index &gt;= <span class="number">20</span>:</div><div class="line">                skip_step = <span class="number">20</span></div><div class="line">            <span class="comment"># 6. run optimization</span></div><div class="line">            sess.run(self.opt)</div><div class="line">            <span class="comment"># 7. add summary info \ save checkpoint every number of certain layers</span></div><div class="line">            <span class="keyword">if</span> (index + <span class="number">1</span>) % skip_step == <span class="number">0</span>:</div><div class="line">                gen_image, total_loss, summary = sess.run([self.input_img,</div><div class="line">                                                        self.total_loss,</div><div class="line">                                                        self.summary_op])</div><div class="line">                gen_image = gen_image + self.vgg.mean_pixels </div><div class="line">                writer.add_summary(summary, global_step=index)</div><div class="line">            <span class="comment">#...</span></div><div class="line">            <span class="keyword">if</span> (index + <span class="number">1</span>) % <span class="number">20</span> == <span class="number">0</span>:</div><div class="line">                saver.save (sess, <span class="string">'checkpoints/style_stranfer/style_transfer'</span>, index)</div></pre></td></tr></table></figure>
</li>
</ol>
<h3 id="个人在做完之后的一点思考"><a href="#个人在做完之后的一点思考" class="headerlink" title="个人在做完之后的一点思考"></a>个人在做完之后的一点思考</h3><p>首先这个任务和传统的学习任务不一样。这次学习的对象是需要生成的图像，待生成的图像像素点作为变量在最优化的时候同时被训练。而网络的模型采用他人训练好的参数。</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/04/12/machinelearning/tensorflow_notes/note_CNN/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="JonassenLi">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/cat.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="JonassenLi">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2018/04/12/machinelearning/tensorflow_notes/note_CNN/" itemprop="url">
                  Convolutional Nerual Networks for Visual Recongnition
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-04-12T22:58:00+08:00">
                2018-04-12
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Convolutional-Nerual-Networks-CNNs-ConvNets"><a href="#Convolutional-Nerual-Networks-CNNs-ConvNets" class="headerlink" title="Convolutional Nerual Networks (CNNs/ConvNets)"></a>Convolutional Nerual Networks (CNNs/ConvNets)</h1><h2 id="Over-view"><a href="#Over-view" class="headerlink" title="Over view"></a>Over view</h2><h3 id="引入"><a href="#引入" class="headerlink" title="引入"></a>引入</h3><p>首先为啥会有CNN这个东西呢？<br>一个普通的神经网络的示意图如下所示<br><img src="./images/tf/simple_neural_net.jpeg" alt=""><br>可见，这种神经网络层与层之间是全连接的，对于minist这种数据集使用，假设输入图像为32<em>32</em>3 = 3072个节点，勉强可以处理。但是对于更大的输入图像，200<em>200</em>3 = 120000个神经元节点，这种神经网络处理起来就比较费力。<br>很显然，这种时候全连接就显得比较无用和浪费，大量的参数不仅难以优化，而且会快速的导致网络的过拟合。  </p>
<h3 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h3><p>卷积神经网络同样由许多层构成，其中主要的有三种：</p>
<ol>
<li>Concoluntional Layer (卷积层)</li>
<li>Polling Layer ()</li>
<li>Full-Connected Layer ()</li>
</ol>
<p>一个较为典型的架构是：[INPUT-CONV-RELU-POOL-FC]</p>
<ul>
<li>INPUT: 3-d [width <em> height </em> color-channels]</li>
<li>CONV : 卷积层</li>
<li>RELU : Rectified Linear Unit (线性整流函数) 常用的有斜坡函数(max(0,x))</li>
<li>POOL : 对输入进行下降抽样（输出向量的前两位小于输入）</li>
<li>FC: 全连接的层  </li>
</ul>
<p>其中只有CONV层和FC层是包含所需要优化的参数的。</p>
<h2 id="Layers"><a href="#Layers" class="headerlink" title="Layers"></a>Layers</h2><h3 id="Convolutional-Layer-卷积层"><a href="#Convolutional-Layer-卷积层" class="headerlink" title="Convolutional Layer (卷积层)"></a>Convolutional Layer (卷积层)</h3><p>我们知道在高维度的输入下，全连接不太实际。取而代之的是，可以对每个节点和输入的某个局部的区域连接。而如何选择这个区域，由一组超参数（hyperparameter）决定，这被称为神经元的（receptive field），也就是filter size.</p>
<h4 id="输出维度（spatial-arrangement）"><a href="#输出维度（spatial-arrangement）" class="headerlink" title="输出维度（spatial-arrangement）"></a>输出维度（spatial-arrangement）</h4><p>输出的空间维度由三个超参数决定：</p>
<ul>
<li>Depth: 输出的深度等于用到的filter的个数。可以理解为：不同的filter试图在数据里面找到不同的特征。</li>
<li>Stride: Stride可以理解为对filter滑动的间距。当Stride较大的时候，输出的维度较小。（通常情况下1、2）</li>
<li>Zero-padding: 有时候在特定Stride值下，不能整除的时候周围输入就要补零。</li>
</ul>
<p>$W =$ input volume size<br>$P =$ receptive field size of the conv layer nerons<br>$S =$ stride<br>$P =$ amount of zero-padding<br>则有：<br>$(W-F+2P)/S + 1$则为一个filter所对应的CONV Layer的节点数。</p>
<h4 id="参数共享（parameter-sharing）"><a href="#参数共享（parameter-sharing）" class="headerlink" title="参数共享（parameter sharing）"></a>参数共享（parameter sharing）</h4><p>在Conv Layer Local connectivity的情况下，假设输入向量大小为[a<em> b </em> c], 输出[x <em> y</em> z], filter [n <em> m </em> c]。那么Conv Layer一共有xyz个节点，每一个节点有nmc个参数，一共有xyznmc个参数，取x = 55, y = 55, z = 96,n = 11,m = 11, c = 3。这种数量级仍然是难以接受的。</p>
<p>可以通过一个合理的假设大量减少参数的数量，可以认为如果某个特征在某一点是有效的，那么在其它点其是同样有效的。也就是说，限制Conv layer在每一个filter（depth）下的神经元使用同样的参数和bias，总的参数数量可以快速减少到zmnc。（在back propogation当中，同意深度下使用相同参数的神经元的贡献会被相加）</p>
<p>Conv Layer 的计算过程如图所示：<br><img src="./images/tf/convolution.png" alt=""></p>
<h4 id="Two-key-insights："><a href="#Two-key-insights：" class="headerlink" title="Two key insights："></a>Two key insights：</h4><p>关于CONV Layer的两个关键点<br>1) Features are hierarchical<br>Composing high-complexity features out of low-complexity features is more<br>efficient than learning high-complexity features directly.<br>e.g.: having an “circle” detector is useful for detecting faces… and basketballs<br>2) Features are translationally invariant<br>If a feature is useful to compute at (x, y) it is useful to compute that feature at<br>(x’, y’) as well</p>
<p>ps: 为何叫卷积层呢：因为其与两个信号的卷积类似。  </p>
<h3 id="Pooling-Layer-不知道咋翻译"><a href="#Pooling-Layer-不知道咋翻译" class="headerlink" title="Pooling Layer (不知道咋翻译..)"></a>Pooling Layer (不知道咋翻译..)</h3><p>Pooling Layer常被加在连续的Conv Layer当中，它的主要作用是逐步减少空间大小来减少参数的数量，从而控制过拟合。  </p>
<p>Pooling层独立的作用于各个depth slice。</p>
<p>一个常见的例子是使用2*2的filter，stride为2,使用max function，取四激励中最大的，从而忽略掉75%的激励</p>
<p>当然还有一些其它pooling的方法，如average pooling和L2-norm pooling在此mark以后深入研究。</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/04/11/machinelearning/tensorflow_notes/note3/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="JonassenLi">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/cat.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="JonassenLi">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2018/04/11/machinelearning/tensorflow_notes/note3/" itemprop="url">
                  TensorFlow 学习笔记3 Manage EXperiments
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-04-11T20:59:00+08:00">
                2018-04-11
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Tensorflow-学习笔记-3"><a href="#Tensorflow-学习笔记-3" class="headerlink" title="Tensorflow 学习笔记 #3"></a>Tensorflow 学习笔记 #3</h1><p><strong>keywords</strong> :  model base, variable sharing, model sharing</p>
<h2 id="构建tensorflow模型的一般步骤"><a href="#构建tensorflow模型的一般步骤" class="headerlink" title="构建tensorflow模型的一般步骤"></a>构建tensorflow模型的一般步骤</h2><p>Phase1: <strong>assenmble graph</strong> </p>
<ol>
<li>Import Data</li>
<li>Define the weigths</li>
<li>Define the inferece model</li>
<li>Define loss function</li>
<li>Define optimizer</li>
</ol>
<p>Phase2: <strong>execute the computation</strong></p>
<ol>
<li>initialize all model variables for the first time</li>
<li>Initialize iterator / feed training data</li>
<li>Excecute the inference model on the training data</li>
<li>compute cost</li>
<li>Adjust model parameters to minimize cost </li>
</ol>
<p>利用python面向对象的性质为自己的模型简历一个类：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Model</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, params)</span>:</span></div><div class="line">        <span class="keyword">pass</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_import_data</span><span class="params">(self)</span>:</span></div><div class="line">        <span class="string">""" Step 1: import data """</span></div><div class="line">        <span class="keyword">pass</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_create_embedding</span><span class="params">(self)</span>:</span></div><div class="line">        <span class="string">""" Step 2: in word2vec, it's actually the weights that we care about """</span></div><div class="line">        <span class="keyword">pass</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_create_loss</span><span class="params">(self)</span>:</span></div><div class="line">        <span class="string">""" Step 3 + 4: define the inference + the loss function """</span></div><div class="line">        <span class="keyword">pass</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_create_optimizer</span><span class="params">(self)</span>:</span></div><div class="line">        <span class="string">""" Step 5: define optimizer """</span></div><div class="line">        <span class="keyword">pass</span></div></pre></td></tr></table></figure></p>
<h2 id="Variable-Sharing"><a href="#Variable-Sharing" class="headerlink" title="Variable Sharing"></a>Variable Sharing</h2><h3 id="Name-Scope"><a href="#Name-Scope" class="headerlink" title="Name Scope"></a>Name Scope</h3><pre><code>为了能够在tensor board上较为清晰的辨识出节点之间的关系，引入name_scope可将其分组
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">with</span> tf.name_scope(name_of_that_scope):</div><div class="line">    <span class="comment"># declare op1</span></div><div class="line">    <span class="comment"># declare op2</span></div><div class="line">    <span class="comment"># declare op3</span></div></pre></td></tr></table></figure>
<h3 id="Variable-Scope"><a href="#Variable-Scope" class="headerlink" title="Variable Scope"></a>Variable Scope</h3><pre><code>使用Varibale scope来做到变量共享，在variable_scope中使用get_variable方法来获取之前创建的变量而不是新的一个变量
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"xxx"</span>) <span class="keyword">as</span> scope:</div><div class="line">    <span class="comment"># a = tf.get_variable("x",.)..</span></div></pre></td></tr></table></figure>
<h2 id="tensorflow-实验管理"><a href="#tensorflow-实验管理" class="headerlink" title="tensorflow 实验管理"></a>tensorflow 实验管理</h2><h3 id="使用checkpoint保存训练中间结果"><a href="#使用checkpoint保存训练中间结果" class="headerlink" title="使用checkpoint保存训练中间结果"></a>使用checkpoint保存训练中间结果</h3><p>对于一个需要较长时间训练的模型来说，断点恢复能力是十分必要的。<br>tensorflow也设置了相应的机制，即为checkpoint，可以用来周期性的保存当前模型的参数等数据。<br>实现这一点的是tf.train.Sacer() 类，它会将图的变量保存在二进制文件当中。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">tf.train.Saver.save(</div><div class="line">    sess,</div><div class="line">    save_path,</div><div class="line">    global_step=<span class="keyword">None</span>,</div><div class="line">    latest_filename=<span class="keyword">None</span>,</div><div class="line">    meta_graph_suffix=<span class="string">'meta'</span>,</div><div class="line">    write_meta_graph=<span class="keyword">True</span>,</div><div class="line">    write_state=<span class="keyword">True</span></div><div class="line">)</div></pre></td></tr></table></figure></p>
<p>常用的保存checkpoint的方法如下所示：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># define model</span></div><div class="line"></div><div class="line"><span class="comment"># create a saver object</span></div><div class="line">saver = tf.train.Saver()</div><div class="line"></div><div class="line"><span class="comment"># launch a session to execute the computation</span></div><div class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">    <span class="comment"># actual training loop</span></div><div class="line">    <span class="keyword">for</span> step <span class="keyword">in</span> range(training_steps): </div><div class="line">	sess.run([optimizer])</div><div class="line">	<span class="keyword">if</span> (step + <span class="number">1</span>) % <span class="number">1000</span> == <span class="number">0</span>:</div><div class="line">	   saver.save(sess, <span class="string">'checkpoint_directory/model_name'</span>, global_step=global_step)</div><div class="line">``` </div><div class="line">这里的global_step是一个用来记录图训练了多少步的变量，创建其的时候需要设置其不能被训练。</div><div class="line">（optimizer默认训练所有变量）</div><div class="line">```python</div><div class="line">global_step = tf.Variable(<span class="number">0</span>, dtype=tf.int32, trainable=<span class="keyword">False</span>, name=<span class="string">'global_step'</span>)</div></pre></td></tr></table></figure></p>
<p>optimizer一般也接收一个global_step变量的输入，每一次优化更新之后会将global_step的值自增<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">optimizer = tf.train.GradientDescentOptimizer(lr).minimize(loss,global_step=global_step)</div></pre></td></tr></table></figure></p>
<p>tensorflow还支持在一个文件夹里面找checkpoint,如果有合法的，恢复checkpoint,否则继续执行<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">ckpt = tf.train.get_checkpoint_state(os.path.dirname(<span class="string">'checkpoints/checkpoint'</span>))</div><div class="line"><span class="keyword">if</span> ckpt <span class="keyword">and</span> ckpt.model_checkpoint_path:</div><div class="line">     saver.restore(sess, ckpt.model_checkpoint_path)</div></pre></td></tr></table></figure></p>
<h3 id="使用tf-summary可视化训练数据"><a href="#使用tf-summary可视化训练数据" class="headerlink" title="使用tf.summary可视化训练数据"></a>使用tf.summary可视化训练数据</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_create_summaries</span><span class="params">(self)</span>:</span></div><div class="line">     <span class="keyword">with</span> tf.name_scope(<span class="string">"summaries"</span>):</div><div class="line">            tf.summary.scalar(<span class="string">"loss"</span>, self.loss)</div><div class="line">            tf.summary.scalar(<span class="string">"accuracy"</span>, self.accuracy)            </div><div class="line">            tf.summary.histogram(<span class="string">"histogram loss"</span>, self.loss)</div><div class="line">            <span class="comment"># because you have several summaries, we should merge them all</span></div><div class="line">            <span class="comment"># into one op to make it easier to manage</span></div><div class="line">            self.summary_op = tf.summary.merge_all()</div></pre></td></tr></table></figure>
<p>summary_op和其它operation一样，需要在sess中运行得到结果。<br>得到结果之后使用add_summary把结果写入writer当中，就可以在tensorbord中看到add_summary的图的各种曲线啦<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">writer.add_summary(summary, global_step=step)</div></pre></td></tr></table></figure></p>
<p>summary的用法参考<a href="https://www.tensorflow.org/programmers_guide/summaries_and_tensorboard" target="_blank" rel="external">这里</a></p>
<h3 id="control-randomization"><a href="#control-randomization" class="headerlink" title="control randomization"></a>control randomization</h3><h3 id="Auto-diff"><a href="#Auto-diff" class="headerlink" title="Auto diff"></a>Auto diff</h3>
          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/03/31/machinelearning/tensorflow_notes/tf_minist/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="JonassenLi">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/cat.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="JonassenLi">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2018/03/31/machinelearning/tensorflow_notes/tf_minist/" itemprop="url">
                  Using tensorflow on Minist
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-03-31T11:31:00+08:00">
                2018-03-31
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="使用卷积神经网络预测minist手写数字"><a href="#使用卷积神经网络预测minist手写数字" class="headerlink" title="使用卷积神经网络预测minist手写数字"></a>使用卷积神经网络预测minist手写数字</h1><p>minist是一个入门的标准cv集。<br>详细注释见minist当中注释<br>直接上代码<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div><div class="line">140</div><div class="line">141</div><div class="line">142</div><div class="line">143</div><div class="line">144</div><div class="line">145</div><div class="line">146</div></pre></td><td class="code"><pre><div class="line"><span class="string">"""Convolutional Neural Network Estimator for MNIST, built with tf.layers."""</span></div><div class="line"></div><div class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> absolute_import</div><div class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> division</div><div class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</div><div class="line"></div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"></div><div class="line">tf.logging.set_verbosity(tf.logging.INFO)</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">cnn_model_fn</span><span class="params">(features, labels, mode)</span>:</span></div><div class="line">  <span class="string">"""Model function for CNN."""</span></div><div class="line">  <span class="comment"># Input Layer</span></div><div class="line">  <span class="comment"># Reshape X to 4-D tensor: [batch_size, width, height, channels]</span></div><div class="line">  <span class="comment"># MNIST images are 28x28 pixels, and have one color channel</span></div><div class="line">  input_layer = tf.reshape(features[<span class="string">"x"</span>], [<span class="number">-1</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>])</div><div class="line"></div><div class="line">  <span class="comment"># Convolutional Layer #1</span></div><div class="line">  <span class="comment"># Computes 32 features using a 5x5 filter with ReLU activation.</span></div><div class="line">  <span class="comment"># Padding is added to preserve width and height.</span></div><div class="line">  <span class="comment"># Input Tensor Shape: [batch_size, 28, 28, 1]</span></div><div class="line">  <span class="comment"># Output Tensor Shape: [batch_size, 28, 28, 32]</span></div><div class="line">  conv1 = tf.layers.conv2d(</div><div class="line">      inputs=input_layer,</div><div class="line">      filters=<span class="number">32</span>,</div><div class="line">      kernel_size=[<span class="number">5</span>, <span class="number">5</span>],</div><div class="line">      padding=<span class="string">"same"</span>,</div><div class="line">      activation=tf.nn.relu)</div><div class="line"></div><div class="line">  <span class="comment"># Pooling Layer #1</span></div><div class="line">  <span class="comment"># First max pooling layer with a 2x2 filter and stride of 2</span></div><div class="line">  <span class="comment"># Input Tensor Shape: [batch_size, 28, 28, 32]</span></div><div class="line">  <span class="comment"># Output Tensor Shape: [batch_size, 14, 14, 32]</span></div><div class="line">  pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[<span class="number">2</span>, <span class="number">2</span>], strides=<span class="number">2</span>)</div><div class="line"></div><div class="line">  <span class="comment"># Convolutional Layer #2</span></div><div class="line">  <span class="comment"># Computes 64 features using a 5x5 filter.</span></div><div class="line">  <span class="comment"># Padding is added to preserve width and height.</span></div><div class="line">  <span class="comment"># Input Tensor Shape: [batch_size, 14, 14, 32]</span></div><div class="line">  <span class="comment"># Output Tensor Shape: [batch_size, 14, 14, 64]</span></div><div class="line">  conv2 = tf.layers.conv2d(</div><div class="line">      inputs=pool1,</div><div class="line">      filters=<span class="number">64</span>,</div><div class="line">      kernel_size=[<span class="number">5</span>, <span class="number">5</span>],</div><div class="line">      padding=<span class="string">"same"</span>,</div><div class="line">      activation=tf.nn.relu)</div><div class="line"></div><div class="line">  <span class="comment"># Pooling Layer #2</span></div><div class="line">  <span class="comment"># Second max pooling layer with a 2x2 filter and stride of 2</span></div><div class="line">  <span class="comment"># Input Tensor Shape: [batch_size, 14, 14, 64]</span></div><div class="line">  <span class="comment"># Output Tensor Shape: [batch_size, 7, 7, 64]</span></div><div class="line">  pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[<span class="number">2</span>, <span class="number">2</span>], strides=<span class="number">2</span>)</div><div class="line"></div><div class="line">  <span class="comment"># Flatten tensor into a batch of vectors</span></div><div class="line">  <span class="comment"># Input Tensor Shape: [batch_size, 7, 7, 64]</span></div><div class="line">  <span class="comment"># Output Tensor Shape: [batch_size, 7 * 7 * 64]</span></div><div class="line">  pool2_flat = tf.reshape(pool2, [<span class="number">-1</span>, <span class="number">7</span> * <span class="number">7</span> * <span class="number">64</span>])</div><div class="line"></div><div class="line">  <span class="comment"># Dense Layer</span></div><div class="line">  <span class="comment"># Densely connected layer with 1024 neurons</span></div><div class="line">  <span class="comment"># Input Tensor Shape: [batch_size, 7 * 7 * 64]</span></div><div class="line">  <span class="comment"># Output Tensor Shape: [batch_size, 1024]</span></div><div class="line">  dense = tf.layers.dense(inputs=pool2_flat, units=<span class="number">1024</span>, activation=tf.nn.relu)</div><div class="line"></div><div class="line">  <span class="comment"># Add dropout operation; 0.6 probability that element will be kept</span></div><div class="line">  dropout = tf.layers.dropout(</div><div class="line">      inputs=dense, rate=<span class="number">0.4</span>, training=mode == tf.estimator.ModeKeys.TRAIN)</div><div class="line"></div><div class="line">  <span class="comment"># Logits layer</span></div><div class="line">  <span class="comment"># Input Tensor Shape: [batch_size, 1024]</span></div><div class="line">  <span class="comment"># Output Tensor Shape: [batch_size, 10]</span></div><div class="line">  logits = tf.layers.dense(inputs=dropout, units=<span class="number">10</span>)</div><div class="line"></div><div class="line">  predictions = &#123;</div><div class="line">      <span class="comment"># Generate predictions (for PREDICT and EVAL mode)</span></div><div class="line">      <span class="string">"classes"</span>: tf.argmax(input=logits, axis=<span class="number">1</span>),</div><div class="line">      <span class="comment"># Add `softmax_tensor` to the graph. It is used for PREDICT and by the</span></div><div class="line">      <span class="comment"># `logging_hook`.</span></div><div class="line">      <span class="string">"probabilities"</span>: tf.nn.softmax(logits, name=<span class="string">"softmax_tensor"</span>)</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">if</span> mode == tf.estimator.ModeKeys.PREDICT:</div><div class="line">    <span class="keyword">return</span> tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)</div><div class="line"></div><div class="line">  <span class="comment"># Calculate Loss (for both TRAIN and EVAL modes)</span></div><div class="line">  loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)</div><div class="line"></div><div class="line">  <span class="comment"># Configure the Training Op (for TRAIN mode)</span></div><div class="line">  <span class="keyword">if</span> mode == tf.estimator.ModeKeys.TRAIN:</div><div class="line">    optimizer = tf.train.GradientDescentOptimizer(learning_rate=<span class="number">0.001</span>)</div><div class="line">    train_op = optimizer.minimize(</div><div class="line">        loss=loss,</div><div class="line">        global_step=tf.train.get_global_step())</div><div class="line">    <span class="keyword">return</span> tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)</div><div class="line"></div><div class="line">  <span class="comment"># Add evaluation metrics (for EVAL mode)</span></div><div class="line">  eval_metric_ops = &#123;</div><div class="line">      <span class="string">"accuracy"</span>: tf.metrics.accuracy(</div><div class="line">          labels=labels, predictions=predictions[<span class="string">"classes"</span>])&#125;</div><div class="line">  <span class="keyword">return</span> tf.estimator.EstimatorSpec(</div><div class="line">      mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">(unused_argv)</span>:</span></div><div class="line">  <span class="comment"># Load training and eval data</span></div><div class="line">  mnist = tf.contrib.learn.datasets.load_dataset(<span class="string">"mnist"</span>)</div><div class="line">  train_data = mnist.train.images  <span class="comment"># Returns np.array</span></div><div class="line">  train_labels = np.asarray(mnist.train.labels, dtype=np.int32)</div><div class="line">  eval_data = mnist.test.images  <span class="comment"># Returns np.array</span></div><div class="line">  eval_labels = np.asarray(mnist.test.labels, dtype=np.int32)</div><div class="line"></div><div class="line">  <span class="comment"># Create the Estimator</span></div><div class="line">  mnist_classifier = tf.estimator.Estimator(</div><div class="line">      model_fn=cnn_model_fn, model_dir=<span class="string">"/tmp/mnist_convnet_model"</span>)</div><div class="line"></div><div class="line">  <span class="comment"># Set up logging for predictions</span></div><div class="line">  <span class="comment"># Log the values in the "Softmax" tensor with label "probabilities"</span></div><div class="line">  tensors_to_log = &#123;<span class="string">"probabilities"</span>: <span class="string">"softmax_tensor"</span>&#125;</div><div class="line">  logging_hook = tf.train.LoggingTensorHook(</div><div class="line">      tensors=tensors_to_log, every_n_iter=<span class="number">50</span>)</div><div class="line"></div><div class="line">  <span class="comment"># Train the model</span></div><div class="line">  train_input_fn = tf.estimator.inputs.numpy_input_fn(</div><div class="line">      x=&#123;<span class="string">"x"</span>: train_data&#125;,</div><div class="line">      y=train_labels,</div><div class="line">      batch_size=<span class="number">100</span>,</div><div class="line">      num_epochs=<span class="keyword">None</span>,</div><div class="line">      shuffle=<span class="keyword">True</span>)</div><div class="line">  mnist_classifier.train(</div><div class="line">      input_fn=train_input_fn,</div><div class="line">      steps=<span class="number">20000</span>,</div><div class="line">      hooks=[logging_hook])</div><div class="line"></div><div class="line">  <span class="comment"># Evaluate the model and print results</span></div><div class="line">  eval_input_fn = tf.estimator.inputs.numpy_input_fn(</div><div class="line">      x=&#123;<span class="string">"x"</span>: eval_data&#125;,</div><div class="line">      y=eval_labels,</div><div class="line">      num_epochs=<span class="number">1</span>,</div><div class="line">      shuffle=<span class="keyword">False</span>)</div><div class="line">  eval_results = mnist_classifier.evaluate(input_fn=eval_input_fn)</div><div class="line">  print(eval_results)</div><div class="line"></div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</div><div class="line">  tf.app.run()</div></pre></td></tr></table></figure></p>
<p>目前我没弄懂的一点在于，这个框架没用session 也没用eager mode是怎么跑起来的， 如果我想可视化地观察一下这个卷积神经网络的结构又该怎么做呢？</p>
<p>有点明白了，官方的doc当中有这么一句话：<br>Estimators build the graph for you. In other words, you don’t have to build the graph.  </p>
<p>其实在一开始应该早就说明白过了，tensorflow的api是分层级的:<br><img src="./images/tf/tf_api.png" alt=""></p>
<p>如上图estimator 和eager mode是同一层级的，它会自动的帮你构建整个图</p>
<p>TODO: 试着跑一遍代码，用log记录下图的结构，然后在tensorboard里面打开康康。  </p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/03/30/machinelearning/tensorflow_notes/note2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="JonassenLi">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/cat.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="JonassenLi">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2018/03/30/machinelearning/tensorflow_notes/note2/" itemprop="url">
                  TensorFlow 学习笔记2
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-03-30T11:31:00+08:00">
                2018-03-30
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="TensorFlow-学习笔记-2"><a href="#TensorFlow-学习笔记-2" class="headerlink" title="TensorFlow 学习笔记 #2"></a>TensorFlow 学习笔记 #2</h1><p>先来看一个简单的线性回归的代码例子，再来看在其基础上可以做出什么改进<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"></div><div class="line"><span class="keyword">import</span> utils</div><div class="line"></div><div class="line">DATA_FILE = <span class="string">"data/birth_life_2010.txt"</span></div><div class="line"></div><div class="line"><span class="comment"># Step 1: read in data from the .txt file</span></div><div class="line"><span class="comment"># data is a numpy array of shape (190, 2), each row is a datapoint</span></div><div class="line">data, n_samples = utils.read_birth_life_data(DATA_FILE)</div><div class="line"></div><div class="line"><span class="comment"># Step 2: create placeholders for X (birth rate) and Y (life expectancy)</span></div><div class="line">X = tf.placeholder(tf.float32, name=<span class="string">'X'</span>)</div><div class="line">Y = tf.placeholder(tf.float32, name=<span class="string">'Y'</span>)</div><div class="line"></div><div class="line"><span class="comment"># Step 3: create weight and bias, initialized to 0</span></div><div class="line">w = tf.get_variable(<span class="string">'weights'</span>, initializer=tf.constant(<span class="number">0.0</span>))</div><div class="line">b = tf.get_variable(<span class="string">'bias'</span>, initializer=tf.constant(<span class="number">0.0</span>))</div><div class="line"></div><div class="line"><span class="comment"># Step 4: construct model to predict Y (life expectancy from birth rate)</span></div><div class="line">Y_predicted = w * X + b </div><div class="line"></div><div class="line"><span class="comment"># Step 5: use the square error as the loss function</span></div><div class="line">loss = tf.square(Y - Y_predicted, name=<span class="string">'loss'</span>)</div><div class="line"></div><div class="line"><span class="comment"># Step 6: using gradient descent with learning rate of 0.01 to minimize loss</span></div><div class="line">optimizer = tf.train.GradientDescentOptimizer(learning_rate=<span class="number">0.001</span>).minimize(loss)</div><div class="line"> </div><div class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">	<span class="comment"># Step 7: initialize the necessary variables, in this case, w and b</span></div><div class="line">	sess.run(tf.global_variables_initializer()) </div><div class="line">	</div><div class="line">	<span class="comment"># Step 8: train the model</span></div><div class="line">	<span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">100</span>): <span class="comment"># run 100 epochs</span></div><div class="line">		<span class="keyword">for</span> x, y <span class="keyword">in</span> data:</div><div class="line">			<span class="comment"># Session runs train_op to minimize loss</span></div><div class="line">			sess.run(optimizer, feed_dict=&#123;X: x, Y:y&#125;) </div><div class="line">	</div><div class="line">	<span class="comment"># Step 9: output the values of w and b</span></div><div class="line">	w_out, b_out = sess.run([w, b])</div></pre></td></tr></table></figure></p>
<h2 id="tensorflow-控制流"><a href="#tensorflow-控制流" class="headerlink" title="tensorflow 控制流"></a>tensorflow 控制流</h2><p>观察上面线性回归所使用的loss function，是个简单的二次函数<br>分析离群点，假设有一个离样本较远的离群点，那么这个离群点造成的loss fuction上的损失较大，会大大影响整个模型的建模。</p>
<p><strong>使用<a href="https://en.wikipedia.org/wiki/Huber_loss" target="_blank" rel="external">huber loss</a>代替原来简单的loss fuction</strong><br>其定义如下所示    </p>
<p>$$<br>L_\delta(y,f(x))=\left{<br>\begin{array}{ll}<br>\frac12(y-f(x))^2,&amp;\textrm{for }|y-f(x)|\leq\delta\\<br>\delta\cdot(|y-f(x)|-\delta/2),&amp; \textrm{otherwise.}<br>\end{array}<br>\right.<br>$$</p>
<p>Huber loss给离群点设置了相对更小的权重,因而提升了拟合的效果。</p>
<p>一个显然的事实是由于tensorflow 定义和执行的分离，我们不能用python的条件分支语句来控制optimizer使用哪一种loss function,tensor flow提供了分支控制的方法</p>
<table>
<thead>
<tr>
<th style="text-align:left">Ops</th>
<th style="text-align:left">Methods</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">Control Flow Ops</td>
<td style="text-align:left">tf.count_up_to, tf.cond, tf.case, tf.while_loop, tf.group …</td>
</tr>
<tr>
<td style="text-align:left">Comparison Ops</td>
<td style="text-align:left">tf.equal, tf.not_equal, tf.less, tf.greater, tf.where, …</td>
</tr>
<tr>
<td style="text-align:left">Logical Ops</td>
<td style="text-align:left">tf.logical_and, tf.logical_not, tf.logical_or, tf.logical_xor</td>
</tr>
<tr>
<td style="text-align:left">Debugging Ops</td>
<td style="text-align:left">tf.is_finite, tf.is_inf, tf.is_nan, tf.Assert, tf.Print, …</td>
</tr>
</tbody>
</table>
<p>huber_loss：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">huber_loss</span><span class="params">(labels, predictions, delta=<span class="number">14.0</span>)</span>:</span></div><div class="line">    residual = tf.abs(labels - predictions)</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">f1</span><span class="params">()</span>:</span> <span class="keyword">return</span> <span class="number">0.5</span> * tf.square(residual)</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">f2</span><span class="params">()</span>:</span> <span class="keyword">return</span> delta * residual - <span class="number">0.5</span> * tf.square(delta)</div><div class="line">    <span class="keyword">return</span> tf.cond(residual &lt; delta, f1, f2)</div></pre></td></tr></table></figure></p>
<h2 id="tensorflow-输入"><a href="#tensorflow-输入" class="headerlink" title="tensorflow 输入"></a>tensorflow 输入</h2><h3 id="placeholder-amp-feed-dict"><a href="#placeholder-amp-feed-dict" class="headerlink" title="placeholder &amp; feed_dict"></a>placeholder &amp; feed_dict</h3><p>note1跳过了对tensorflow基本输入方式的叙述。实际上由于graph在定义的时候不需要考虑实际输入数据的特性。一般创建输入变量的时候实际上是为要输入的变量预留位置，使用tf.placeholder定义,如下是一个使用的例子<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">a = tf.placeholder(dtype, shape=<span class="keyword">None</span>, name=<span class="keyword">None</span>)</div><div class="line">...</div><div class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">   sess.run(something, feed_dict = &#123;a:[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]&#125;)</div></pre></td></tr></table></figure></p>
<p>shape参数制定了传入的tensor的结构，shape为None意味着任意结构的tensor都能被接收（可能潜在地会引入bug）</p>
<h3 id="tf-data"><a href="#tf-data" class="headerlink" title="tf.data"></a>tf.data</h3><p>placeholder让数据的处理和带入图中运算分开，在tensorflow框架之外完成（完全可以用numpy等工具处理），不过这样带来的不好的地方之一在于，数据处理被放在了python的单一线程当中，会让数据处理较慢。（大量数据要从外部一个个装载到place_holder处）  </p>
<p>如上述代码当中看起来就不优雅的一段：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">100</span>): <span class="comment"># run 100 epochs</span></div><div class="line">	<span class="keyword">for</span> x, y <span class="keyword">in</span> data:</div><div class="line">		<span class="comment"># Session runs train_op to minimize loss</span></div><div class="line">		sess.run(optimizer, feed_dict=&#123;X: x, Y:y&#125;)</div></pre></td></tr></table></figure></p>
<p>将数据分100次载入place_holder当中实际上较大的拖慢了程序的速度。还需要考虑的是在并行计算的时候载入feed_dict可能阻碍了其它操作的执行。</p>
<p>tensorflow提供的解决方案是将数据存储在tf.data.Dataset object当中<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">tf.data.Dataset.from_tensor_slices((features, labels))</div><div class="line"><span class="comment"># can use numpy arrays as features and labels</span></div></pre></td></tr></table></figure></p>
<p>之后使用迭代器来访问dataset当中的每一个数据<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># we use make_initializable_iterator for multiple epochs</span></div><div class="line">iterator = dataset.make_initializable_iterator()</div><div class="line">X, Y = iterator.get_next() </div><div class="line">···</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">100</span>): </div><div class="line">        <span class="comment"># reset where iterator point to</span></div><div class="line">        sess.run(iterator.initializer)</div><div class="line">        total_loss = <span class="number">0</span></div><div class="line">        <span class="keyword">try</span>:</div><div class="line">            <span class="keyword">while</span> <span class="keyword">True</span>:</div><div class="line">                sess.run([optimizer]) </div><div class="line">        <span class="keyword">except</span> tf.errors.OutOfRangeError:</div><div class="line">            <span class="keyword">pass</span></div></pre></td></tr></table></figure></p>
<p>dataset 也支持许多原生的对数据集的操作来改变数据集或是生成新的数据集<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">dataset = dataset.shuffle(<span class="number">1000</span>)</div><div class="line">dataset = dataset.repeat(<span class="number">100</span>)</div><div class="line">dataset = dataset.batch(<span class="number">128</span>)</div><div class="line">dataset = dataset.map(<span class="keyword">lambda</span> x: tf.one_hot(x, <span class="number">10</span>)) </div><div class="line"><span class="comment"># convert each element of dataset to one_hot vector</span></div></pre></td></tr></table></figure></p>
<h2 id="Optimizers"><a href="#Optimizers" class="headerlink" title="Optimizers"></a>Optimizers</h2><p>默认情况下optimizer在每一轮迭代的过程中自动更新loss function所依赖的所有变量，若有不想更新的变量，在定义的时候加上参数trainable=False</p>
<p>(to do: add contont about more detailed control of model trains using tf.gradient)</p>
<h2 id="Refs"><a href="#Refs" class="headerlink" title="Refs"></a>Refs</h2><p><a href="https://docs.google.com/document/d/1kMGs68rIHWHifBiqlU3j_2ZkrNj9RquGTe8tJ7eR1sE/edit#" target="_blank" rel="external">03_Lecture note_Linear and Logistic Regression</a><br><a href="https://en.wikipedia.org/wiki/Huber_loss" target="_blank" rel="external">Huber Loss</a></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/03/26/machinelearning/tensorflow_notes/note1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="JonassenLi">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/cat.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="JonassenLi">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2018/03/26/machinelearning/tensorflow_notes/note1/" itemprop="url">
                  TensorFlow 学习笔记1
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-03-26T10:10:32+08:00">
                2018-03-26
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="TensorFlow-学习笔记-1"><a href="#TensorFlow-学习笔记-1" class="headerlink" title="TensorFlow 学习笔记 #1"></a>TensorFlow 学习笔记 #1</h1><h2 id="TensorFlow-Basics"><a href="#TensorFlow-Basics" class="headerlink" title="TensorFlow Basics"></a>TensorFlow Basics</h2><h3 id="Graphs-and-Sessions"><a href="#Graphs-and-Sessions" class="headerlink" title="Graphs and Sessions"></a>Graphs and Sessions</h3><p>首先，需要和普通python程序区别出来的是，tensorflow将计算的定义和具体执行过程分离开来，个人认为这有点像函数式中的求值运算。  </p>
<p>不过tensorflow的特点在于其把所依赖的所有计算转换成一个数据流图（dag）</p>
<p><img src="./images/tf/tensor_data_flow_graph.png" alt="数据流图"></p>
<ol>
<li>根据输入构成数据流图</li>
<li>创建会话，执行操作</li>
</ol>
<p>用图的优点有如下几个：  </p>
<ol>
<li>能够保存计算结果。只会运行你所期望得到值的子图。</li>
<li>易于分布任务，进行分布式的计算</li>
<li>Break computation into small, differential pieces to facilitate auto-differentiation</li>
<li>Many common machine learning models are taught and visualized as directed graphs</li>
</ol>
<p><strong>何为TensorFlow?</strong><br><strong>Tensor</strong>: An n-dimensional array<br>0-d tensor: scalar (number)<br>1-d tensor: vector<br>2-d tensor: matrix  </p>
<h2 id="Tensorflow-ops"><a href="#Tensorflow-ops" class="headerlink" title="Tensorflow ops"></a>Tensorflow ops</h2><h3 id="TensorBorad"><a href="#TensorBorad" class="headerlink" title="TensorBorad"></a>TensorBorad</h3><p>Tensorborad 使用通过将图的节点信息和图中的操作记入event files当中来完成整个流程的可视化，使用如下代码创建event files以及停止记录<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># use tf.get_default_graph() to get default graph</span></div><div class="line">writer = tf.summary.FileWriter([logdir], [graph])</div><div class="line"><span class="comment"># ...</span></div><div class="line">writer.close()</div></pre></td></tr></table></figure></p>
<p>之后运行python代码并打开tensorboard<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ python3 [my_program.py] </div><div class="line">$ tensorboard --logdir=<span class="string">"./graphs"</span> --port 6006</div></pre></td></tr></table></figure></p>
<p>但是我们此时看到的图每个节点我们无法对上名字，这就要在定义图的时候给出其的名字<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">a = tf.constant(<span class="number">2</span>, name=<span class="string">"a"</span>)</div><div class="line">b = tf.constant(<span class="number">2</span>, name=<span class="string">"b"</span>)</div><div class="line">x = tf.add(a, b, name=<span class="string">"add"</span>)</div></pre></td></tr></table></figure></p>
<h3 id="Some-useful-tricks"><a href="#Some-useful-tricks" class="headerlink" title="Some useful tricks"></a>Some useful tricks</h3><h4 id="查看protobuf"><a href="#查看protobuf" class="headerlink" title="查看protobuf"></a>查看protobuf</h4><p>常数存储在函数的定义当中，通过查看图的protobuf(protocol buffer)来查看图定义当中的内容。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"></div><div class="line">my_const = tf.constant([<span class="number">1.0</span>, <span class="number">2.0</span>], name=<span class="string">"my_const"</span>)</div><div class="line">print(tf.get_default_graph().as_graph_def())</div></pre></td></tr></table></figure></p>
<p>Output :<br><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line">node &#123;</div><div class="line">  name: "my_const"</div><div class="line">  op: "Const"</div><div class="line">  attr &#123;</div><div class="line">    key: "dtype"</div><div class="line">    value &#123;</div><div class="line">      type: DT_FLOAT</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">  attr &#123;</div><div class="line">    key: "value"</div><div class="line">    value &#123;</div><div class="line">      tensor &#123;</div><div class="line">        dtype: DT_FLOAT</div><div class="line">        tensor_shape &#123;</div><div class="line">          dim &#123;</div><div class="line">            size: 2</div><div class="line">          &#125;</div><div class="line">        &#125;</div><div class="line">        tensor_content: "\000\000\200?\000\000\000@"</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line">versions &#123;</div><div class="line">  producer: 24</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<h4 id="变量的声明和初始化"><a href="#变量的声明和初始化" class="headerlink" title="变量的声明和初始化"></a>变量的声明和初始化</h4><p>为了变量共享的方便 官方推荐使用tf.get_variable方法<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">s = tf.get_variable(<span class="string">"scalar"</span>, initializer=tf.constant(<span class="number">2</span>)) </div><div class="line">m = tf.get_variable(<span class="string">"matrix"</span>, initializer=tf.constant([[<span class="number">0</span>, <span class="number">1</span>], [<span class="number">2</span>, <span class="number">3</span>]]))</div><div class="line">W = tf.get_variable(<span class="string">"big_matrix"</span>, shape=(<span class="number">784</span>, <span class="number">10</span>), initializer=tf.zeros_initializer())</div></pre></td></tr></table></figure></p>
<p>同时可以较简单的初始化变量<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">	sess.run(tf.global_variables_initializer())</div></pre></td></tr></table></figure></p>
<h4 id="Assign-a-variable"><a href="#Assign-a-variable" class="headerlink" title="Assign a variable"></a>Assign a variable</h4><p>观察如下程序<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">W = tf.Variable(<span class="number">10</span>)</div><div class="line">W.assign(<span class="number">100</span>)</div><div class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">	sess.run(W.initializer)</div><div class="line">	print(W.eval()) <span class="comment"># &gt;&gt; 10</span></div></pre></td></tr></table></figure></p>
<p>结果输出是10，但为什么不是100呢。注意的是，之前也说过，tensorflow的声明和运行是分离的，W.assign(100)创建了一个assign操作，但是我们并没有运行它，所以应该按照如下写<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">W = tf.Variable(<span class="number">10</span>)</div><div class="line">assign_op = W.assign(<span class="number">100</span>)</div><div class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">	sess.run(assign_op)</div><div class="line">	print(W.eval()) <span class="comment"># &gt;&gt; 100</span></div></pre></td></tr></table></figure></p>
<h4 id="Sessions"><a href="#Sessions" class="headerlink" title="Sessions"></a>Sessions</h4><p>会话独自保存值，因而假如有两个不同的会话对同一个变量进行操作，其得到最终的值也有可能不相同。<br>有时候为了方便可以使用interactive session来隐式地run session<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">sess = tf.InteractiveSession()</div><div class="line">a = tf.constant(<span class="number">5.0</span>)</div><div class="line">b = tf.constant(<span class="number">6.0</span>)</div><div class="line">c = a * b</div><div class="line">print(c.eval()) <span class="comment"># we can use 'c.eval()' without explicitly stating a session</span></div><div class="line">sess.close()</div></pre></td></tr></table></figure></p>
<p>tf.get_default_session()返回当前进程的默认session</p>
<h4 id="the-trap-of-lazy-loading"><a href="#the-trap-of-lazy-loading" class="headerlink" title="the trap of lazy loading"></a>the trap of lazy loading</h4><p>考虑如下代码有什么不好的地方<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">x = tf.Variable(10, name=&apos;x&apos;)</div><div class="line">y = tf.Variable(20, name=&apos;y&apos;)</div><div class="line"></div><div class="line">with tf.Session() as sess:</div><div class="line">	sess.run(tf.global_variables_initializer())</div><div class="line">	writer = tf.summary.FileWriter(&apos;graphs/lazy_loading&apos;, sess.graph)</div><div class="line">	for _ in range(10):</div><div class="line">		sess.run(tf.add(x, y))</div><div class="line">	print(tf.get_default_graph().as_graph_def()) </div><div class="line">	writer.close()</div></pre></td></tr></table></figure></p>
<p>sess.run(tf.add(x, y))这一句会将tf.add(x,y)这个操作创建10次，造成网络的大量冗余<br>考虑解决方案：</p>
<ol>
<li>总是将操作的定义与执行分离开来</li>
<li>使用python的@property来保证你的函数只被调用一次</li>
</ol>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/09/22/acm/开关反转问题/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="JonassenLi">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/cat.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="JonassenLi">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/09/22/acm/开关反转问题/" itemprop="url">
                  开关反转问题
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-09-22T04:19:25+08:00">
                2017-09-22
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cstdio&gt;</span></span></div><div class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></div><div class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cstring&gt;</span></span></div><div class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cmath&gt;</span></span></div><div class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;utility&gt;</span></span></div><div class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;queue&gt;</span></span></div><div class="line"><span class="meta">#<span class="meta-keyword">define</span> MAXN 50</span></div><div class="line"><span class="meta">#<span class="meta-keyword">define</span> sc(x) scanf(<span class="meta-string">"%d"</span>,&amp;(x))</span></div><div class="line"></div><div class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</div><div class="line"></div><div class="line"><span class="keyword">const</span> <span class="keyword">int</span> INF = <span class="number">0x3f3f3f3f</span>;</div><div class="line"><span class="keyword">int</span> a[MAXN][MAXN];</div><div class="line"><span class="keyword">int</span> p[MAXN][MAXN];</div><div class="line"><span class="keyword">int</span> f[MAXN][MAXN][MAXN];</div><div class="line"><span class="keyword">int</span> x[MAXN];</div><div class="line"><span class="keyword">int</span> mark[MAXN];</div><div class="line"><span class="keyword">int</span> M,N;</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">int</span> <span class="title">gauss</span><span class="params">(<span class="keyword">int</span> equ,<span class="keyword">int</span> vars)</span></span></div><div class="line">&#123;</div><div class="line">	<span class="keyword">int</span> i=<span class="number">0</span>,j=<span class="number">0</span>;</div><div class="line">	<span class="built_in">memset</span>(mark,<span class="number">0</span>,<span class="keyword">sizeof</span>(mark));</div><div class="line">	<span class="keyword">for</span>(i=<span class="number">0</span>,j=<span class="number">0</span>; i&lt;equ &amp;&amp; j&lt;vars; i++,j++)&#123;</div><div class="line">		<span class="keyword">int</span> max_r = i;</div><div class="line">		<span class="keyword">for</span>(<span class="keyword">int</span> k=i+<span class="number">1</span>; k&lt;equ; k++)</div><div class="line">			<span class="keyword">if</span>(a[k][j]&gt;a[max_r][j])</div><div class="line">				max_r = k;</div><div class="line">        <span class="keyword">if</span>(max_r!=i)</div><div class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> k=<span class="number">0</span>; k&lt;=vars; k++)</div><div class="line">                swap(a[i][k],a[max_r][k]);</div><div class="line">        <span class="keyword">if</span>(a[i][j]==<span class="number">0</span>)&#123;</div><div class="line">            mark[j] = <span class="number">1</span>;</div><div class="line">            i--;	<span class="keyword">continue</span>;</div><div class="line">        &#125;</div><div class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> k=i+<span class="number">1</span>; k&lt;equ; k++)&#123;</div><div class="line">            <span class="keyword">if</span>(a[k][j])</div><div class="line">                <span class="keyword">for</span>(<span class="keyword">int</span> m=j; m&lt;=vars; m++)</div><div class="line">                    a[k][m] = a[k][m]^a[i][m];</div><div class="line">        &#125;</div><div class="line">	&#125;</div><div class="line">	i--;</div><div class="line">	<span class="keyword">for</span>(<span class="keyword">int</span> k=vars<span class="number">-1</span>; k&gt;=<span class="number">0</span>; k--)&#123;</div><div class="line">		<span class="keyword">if</span>(mark[k])&#123;</div><div class="line">			x[k] = <span class="number">1</span>;	<span class="keyword">continue</span>;</div><div class="line">		&#125;</div><div class="line">		<span class="keyword">if</span>(i&lt;<span class="number">0</span>) <span class="keyword">break</span>;</div><div class="line">		<span class="keyword">int</span> res = <span class="number">0</span>;</div><div class="line">		<span class="keyword">for</span>(<span class="keyword">int</span> m=k+<span class="number">1</span>; m&lt;vars; m++)</div><div class="line">			res ^= a[i][m]&amp;x[m];</div><div class="line">		x[i--] = res;</div><div class="line">	&#125;</div><div class="line">	<span class="keyword">return</span> <span class="number">0</span>;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">int</span> <span class="title">get</span><span class="params">(<span class="keyword">int</span> i,<span class="keyword">int</span> j,<span class="keyword">int</span> k)</span></span></div><div class="line">&#123;</div><div class="line">	<span class="keyword">int</span> ans = f[i][j][k] + f[i<span class="number">-1</span>][j][k] + f[i][j<span class="number">-1</span>][k] + f[i][j+<span class="number">1</span>][k];</div><div class="line">	<span class="keyword">return</span> ans%<span class="number">2</span>;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">int</span> <span class="title">getans</span><span class="params">(<span class="keyword">int</span> i,<span class="keyword">int</span> j)</span></span></div><div class="line">&#123;</div><div class="line">	<span class="keyword">int</span> ans = p[i][j] + p[i<span class="number">-1</span>][j] + p[i][j<span class="number">-1</span>] + p[i][j+<span class="number">1</span>];</div><div class="line">	<span class="keyword">return</span> ans%<span class="number">2</span>;</div><div class="line">&#125;</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></div><div class="line">&#123;</div><div class="line">	<span class="keyword">int</span> t;</div><div class="line">	sc(t);</div><div class="line">	<span class="keyword">while</span>(t--)&#123;</div><div class="line">		sc(N),sc(M);</div><div class="line">		<span class="built_in">memset</span>(a,<span class="number">0</span>,<span class="keyword">sizeof</span>(a));</div><div class="line">		<span class="built_in">memset</span>(p,<span class="number">0</span>,<span class="keyword">sizeof</span>(p));</div><div class="line">		<span class="built_in">memset</span>(f,<span class="number">0</span>,<span class="keyword">sizeof</span>(f));</div><div class="line">		<span class="keyword">for</span>(<span class="keyword">int</span> k=<span class="number">1</span>; k&lt;=M; k++)</div><div class="line">		&#123;</div><div class="line">			f[<span class="number">1</span>][k][k] = <span class="number">1</span>;</div><div class="line">		&#125;</div><div class="line">		<span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">2</span>; i&lt;=N+<span class="number">1</span>; i++)</div><div class="line">			<span class="keyword">for</span>(<span class="keyword">int</span> j=<span class="number">1</span>; j&lt;=M; j++)</div><div class="line">                <span class="keyword">for</span>(<span class="keyword">int</span> k=<span class="number">1</span>; k&lt;=M; k++)</div><div class="line">                    f[i][j][k] = get(i<span class="number">-1</span>,j,k);</div><div class="line"></div><div class="line">		<span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;M; i++)</div><div class="line">			<span class="keyword">for</span>(<span class="keyword">int</span> j=<span class="number">0</span>; j&lt;M; j++)</div><div class="line">				a[i][j] = f[N+<span class="number">1</span>][i+<span class="number">1</span>][j+<span class="number">1</span>];</div><div class="line">		gauss(M,M);</div><div class="line">		<span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">1</span>; i&lt;=M; i++)</div><div class="line">			p[<span class="number">1</span>][i] = x[i<span class="number">-1</span>];</div><div class="line">		<span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">2</span>; i&lt;=N; i++)</div><div class="line">			<span class="keyword">for</span>(<span class="keyword">int</span> j=<span class="number">1</span>; j&lt;=M; j++)</div><div class="line">				p[i][j] = getans(i<span class="number">-1</span>,j);</div><div class="line">		<span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">1</span>; i&lt;=N; i++)&#123;</div><div class="line">			<span class="built_in">printf</span>(<span class="string">"%d"</span>,p[i][<span class="number">1</span>]);</div><div class="line">			<span class="keyword">for</span>(<span class="keyword">int</span> j=<span class="number">2</span>; j&lt;=M; j++)</div><div class="line">				<span class="built_in">printf</span>(<span class="string">" %d"</span>,p[i][j]);</div><div class="line">			<span class="built_in">printf</span>(<span class="string">"\n"</span>);</div><div class="line"> 	    &#125;</div><div class="line">	&#125;</div><div class="line">	<span class="keyword">return</span> <span class="number">0</span>;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/09/21/acm/高斯消元求解带模方程组/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="JonassenLi">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/cat.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="JonassenLi">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/09/21/acm/高斯消元求解带模方程组/" itemprop="url">
                  高斯消元求解带模方程组
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-09-21T02:26:04+08:00">
                2017-09-21
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">in</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/acm/" itemprop="url" rel="index">
                    <span itemprop="name">ACM</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>HDU-5755<br>一类开关问题<br>做法是设第一行各个位置操作为x1~xn次那么就可以最后一直推下去<br>推出第n+1行所需要的操作个数一定都为0,这样就得到了一个n个变量的方程组<br>用高斯消元求出解都推出来就行</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">const</span> <span class="keyword">int</span> MOD = <span class="number">3</span>;</div><div class="line"><span class="keyword">const</span> <span class="keyword">int</span> INF = <span class="number">0x3f3f3f3f</span>;</div><div class="line"><span class="keyword">int</span> a[MAXN][MAXN];</div><div class="line"><span class="keyword">int</span> x[MAXN];</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">inline</span> <span class="keyword">int</span> <span class="title">gcd</span><span class="params">(<span class="keyword">int</span> a,<span class="keyword">int</span> b)</span></span></div><div class="line">&#123;</div><div class="line">	<span class="keyword">int</span> t;</div><div class="line">	<span class="keyword">while</span>(b != <span class="number">0</span>)&#123;</div><div class="line">		t = b;</div><div class="line">		b = a%b;</div><div class="line">		a = t;</div><div class="line">	&#125;</div><div class="line">	<span class="keyword">return</span> a;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">inline</span> <span class="keyword">void</span> <span class="title">exgcd</span><span class="params">(<span class="keyword">int</span> a,<span class="keyword">int</span> b,<span class="keyword">int</span> &amp;x,<span class="keyword">int</span> &amp;y,<span class="keyword">int</span> &amp;d)</span></span></div><div class="line">&#123;</div><div class="line">	<span class="keyword">if</span>(b==<span class="number">0</span>)&#123;</div><div class="line">		x = <span class="number">1</span>; y=<span class="number">0</span>; d = a;	<span class="keyword">return</span>;</div><div class="line">	&#125;</div><div class="line">	exgcd(b,a%b,x,y,d);</div><div class="line">	<span class="keyword">int</span> t = x;</div><div class="line">	x = y;</div><div class="line">	y = t - (a/b)*y;</div><div class="line">	<span class="keyword">return</span>;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">int</span> <span class="title">lcm</span><span class="params">(<span class="keyword">int</span> a,<span class="keyword">int</span> b)</span></span>&#123;</div><div class="line">	<span class="keyword">return</span> a*b/gcd(a,b);</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">int</span> <span class="title">inv</span><span class="params">(<span class="keyword">int</span> a,<span class="keyword">int</span> mod)</span></span></div><div class="line">&#123;</div><div class="line">	<span class="keyword">int</span> x,y,d;</div><div class="line">	exgcd(a,mod,x,y,d);</div><div class="line">	<span class="keyword">return</span> (x+mod)%mod;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">int</span> <span class="title">gauss</span><span class="params">(<span class="keyword">int</span> rows,<span class="keyword">int</span> var)</span></span></div><div class="line">&#123;</div><div class="line">	<span class="keyword">int</span> col,k,max_r;</div><div class="line">	<span class="keyword">for</span>(col=<span class="number">0</span>,k=<span class="number">0</span>; col&lt;var &amp;&amp; k&lt;rows; col++,k++)&#123;</div><div class="line">		max_r = k;</div><div class="line">		<span class="keyword">for</span>(<span class="keyword">int</span> i=k+<span class="number">1</span>; i&lt;rows; i++)</div><div class="line">			<span class="keyword">if</span>(<span class="built_in">abs</span>(a[i][col])&gt;<span class="built_in">abs</span>(a[k][col]))</div><div class="line">				max_r = i;</div><div class="line">		<span class="keyword">if</span>(a[max_r][col]==<span class="number">0</span>)&#123;</div><div class="line">			k--; <span class="keyword">continue</span>;	<span class="comment">//Jump to next variable</span></div><div class="line">		&#125;</div><div class="line">		<span class="keyword">if</span>(max_r!=k)</div><div class="line">			<span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;var+<span class="number">1</span>; i++)</div><div class="line">				swap(a[k][i],a[max_r][i]);</div><div class="line">		<span class="keyword">for</span>(<span class="keyword">int</span> i=k+<span class="number">1</span>; i&lt;rows; i++)&#123;</div><div class="line">			<span class="keyword">if</span>(a[i][col]!=<span class="number">0</span>)&#123;</div><div class="line">				<span class="keyword">int</span> LCM = lcm(a[k][col],a[i][col]);</div><div class="line">				<span class="keyword">int</span> tk = LCM/a[k][col];</div><div class="line">				<span class="keyword">int</span> ti = LCM/a[i][col];</div><div class="line">				<span class="keyword">for</span>(<span class="keyword">int</span> j=col; j&lt;var+<span class="number">1</span>; j++)</div><div class="line">					a[i][j] = ((a[i][j]*ti - a[k][j]*tk)%MOD+MOD)%MOD;</div><div class="line">			&#125;</div><div class="line">		&#125;</div><div class="line">	&#125;</div><div class="line">	<span class="keyword">for</span>(<span class="keyword">int</span> i=k; i&lt;rows; i++)</div><div class="line">		<span class="keyword">if</span>(a[i][col]!=<span class="number">0</span>)</div><div class="line">			<span class="keyword">return</span> <span class="number">-1</span>;				<span class="comment">//</span></div><div class="line">	<span class="keyword">if</span>(rows&lt;var) <span class="keyword">return</span> rows-var;	<span class="comment">//</span></div><div class="line"></div><div class="line">	<span class="keyword">for</span>(<span class="keyword">int</span> i=var<span class="number">-1</span>; i&gt;=<span class="number">0</span>; i--)&#123;</div><div class="line">		<span class="keyword">int</span> temp = a[i][var];</div><div class="line">		<span class="keyword">for</span>(<span class="keyword">int</span> j=i+<span class="number">1</span>; j&lt;var; j++)</div><div class="line">		&#123;</div><div class="line">			<span class="keyword">if</span>(a[i][j]!=<span class="number">0</span>)&#123;</div><div class="line">				temp -= a[i][j]*x[j];</div><div class="line">				temp = (temp%MOD+MOD)%MOD;</div><div class="line">			&#125;</div><div class="line">		&#125;</div><div class="line">		x[i] = (temp*inv(a[i][i],MOD))%MOD;</div><div class="line">	&#125;</div><div class="line">	<span class="keyword">return</span> <span class="number">0</span>;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">int</span> T,N,M;</div><div class="line"><span class="keyword">int</span> m[<span class="number">50</span>][<span class="number">50</span>];</div><div class="line"><span class="keyword">int</span> f[<span class="number">50</span>][<span class="number">50</span>][<span class="number">50</span>];</div><div class="line"><span class="keyword">int</span> p[<span class="number">50</span>][<span class="number">50</span>];</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">int</span> <span class="title">get</span><span class="params">(<span class="keyword">int</span> i,<span class="keyword">int</span> j,<span class="keyword">int</span> k)</span></span></div><div class="line">&#123;</div><div class="line">    <span class="keyword">int</span> ans = (f[i][j<span class="number">-1</span>][k]+<span class="number">2</span>*f[i][j][k]+f[i][j+<span class="number">1</span>][k]+f[i<span class="number">-1</span>][j][k])%MOD;</div><div class="line">    <span class="keyword">return</span> ((<span class="number">3</span>-ans)%MOD+MOD)%MOD;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">int</span> <span class="title">getans</span><span class="params">(<span class="keyword">int</span> i,<span class="keyword">int</span> j)</span></span></div><div class="line">&#123;</div><div class="line">    <span class="keyword">int</span> ans = (p[i][j<span class="number">-1</span>]+<span class="number">2</span>*p[i][j]+p[i][j+<span class="number">1</span>]+p[i<span class="number">-1</span>][j]+m[i][j])%MOD;</div><div class="line">    <span class="keyword">return</span> ((<span class="number">3</span>-ans)%MOD+MOD)%MOD;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></div><div class="line">&#123;</div><div class="line">	sc(T);</div><div class="line">	<span class="keyword">while</span>(T--)&#123;</div><div class="line">		sc(N);	sc(M);</div><div class="line">		<span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">1</span>; i&lt;=N; i++)</div><div class="line">			<span class="keyword">for</span>(<span class="keyword">int</span> j=<span class="number">1</span>; j&lt;=M; j++)</div><div class="line">				sc(m[i][j]);</div><div class="line">		<span class="built_in">memset</span>(a,<span class="number">0</span>,<span class="keyword">sizeof</span>(a));</div><div class="line">        <span class="built_in">memset</span>(f,<span class="number">0</span>,<span class="keyword">sizeof</span>(f));</div><div class="line">        <span class="built_in">memset</span>(p,<span class="number">0</span>,<span class="keyword">sizeof</span>(p));</div><div class="line">		<span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">1</span>; i&lt;=M; i++)</div><div class="line">			f[<span class="number">0</span>][i][i] = <span class="number">1</span>;</div><div class="line">		<span class="keyword">int</span> cur = <span class="number">0</span>;</div><div class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">1</span>; i&lt;=N; i++)&#123;</div><div class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> j=<span class="number">1</span>; j&lt;=M; j++)&#123;</div><div class="line">                <span class="keyword">for</span>(<span class="keyword">int</span> k=<span class="number">1</span>; k&lt;=M; k++)</div><div class="line">                    f[i][j][k] = get(i<span class="number">-1</span>,j,k);</div><div class="line">                f[i][j][M+<span class="number">1</span>] = (<span class="number">3</span> - m[i][j] + get(i<span class="number">-1</span>,j,M+<span class="number">1</span>))%MOD;</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line">		<span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;M; i++)&#123;</div><div class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> j=<span class="number">0</span>; j&lt;M; j++)</div><div class="line">                a[i][j] = f[N][i+<span class="number">1</span>][j+<span class="number">1</span>];</div><div class="line">            a[i][M] = (<span class="number">3</span>-f[N][i+<span class="number">1</span>][M+<span class="number">1</span>])%MOD;</div><div class="line">		&#125;</div><div class="line">        gauss(M,M);</div><div class="line">        <span class="keyword">int</span> ans = <span class="number">0</span>;</div><div class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">1</span>; i&lt;=M; i++)</div><div class="line">            p[<span class="number">1</span>][i] = x[i<span class="number">-1</span>],ans += p[<span class="number">1</span>][i];</div><div class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">2</span>; i&lt;=N; i++)</div><div class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> j=<span class="number">1</span>; j&lt;=M; j++)</div><div class="line">                p[i][j] = getans(i<span class="number">-1</span>,j),ans += p[i][j];</div><div class="line">        <span class="built_in">printf</span>(<span class="string">"%d\n"</span>,ans);</div><div class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">1</span>; i&lt;=N; i++)</div><div class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> j=<span class="number">1</span>; j&lt;=M; j++)</div><div class="line">                <span class="keyword">for</span>(<span class="keyword">int</span> k=<span class="number">0</span>; k&lt;p[i][j]; k++)</div><div class="line">                    <span class="built_in">printf</span>(<span class="string">"%d %d\n"</span>,i,j);</div><div class="line">	&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/4/">4</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>


          
          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      

      <section class="site-overview sidebar-panel sidebar-panel-active">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/cat.jpg"
               alt="JonassenLi" />
          <p class="site-author-name" itemprop="name">JonassenLi</p>
           
              <p class="site-description motion-element" itemprop="description">salty fish jojo23333's personal page</p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives">
                <span class="site-state-item-count">32</span>
                <span class="site-state-item-name">Artikel</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-categories">
              <a href="/categories/index.html">
                <span class="site-state-item-count">4</span>
                <span class="site-state-item-name">Kategorien</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">33</span>
                <span class="site-state-item-name">Tags</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/jojo23333" target="_blank" title="GitHub">
                  
                    <i class="fa fa-fw fa-github"></i>
                  
                  GitHub
                </a>
              </span>
            
          
        </div>

        
        

        
        

        


      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">JonassenLi</span>
</div>


<div class="powered-by">
  Erstellt mit  <a class="theme-link" href="https://hexo.io">Hexo</a>
</div>

<div class="theme-info">
  Theme -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Mist
  </a>
</div>


        

        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    
    
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  




  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.0"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.0"></script>



  



  




	





  





  





  



  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

  


  

</body>
</html>
